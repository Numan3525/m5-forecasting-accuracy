{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from  datetime import datetime, timedelta\nimport gc\nimport numpy as np, pandas as pd\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> This notebook aims to push the public LB under 0.50. Certainly, the competition is not yet at its peak and there clearly remains room for improvement."},{"metadata":{},"cell_type":"markdown","source":"This notebook is based on [m5-first-public-notebook-under-0-50](https://www.kaggle.com/kneroma/m5-first-public-notebook-under-0-50) v.6 by @kkiller \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"CAL_DTYPES={\"event_name_1\": \"category\", \"event_name_2\": \"category\", \"event_type_1\": \"category\", \n         \"event_type_2\": \"category\", \"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n        \"month\": \"int16\", \"year\": \"int16\", \"snap_CA\": \"float32\", 'snap_TX': 'float32', 'snap_WI': 'float32' }\nPRICE_DTYPES = {\"store_id\": \"category\", \"item_id\": \"category\", \"wm_yr_wk\": \"int16\",\"sell_price\":\"float32\" }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.max_columns = 50","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"h = 28 \nmax_lags = 57\ntr_last = 1913\nfday = datetime(2016,4, 25) \nfday","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_dt(is_train = True, nrows = None, first_day = 1200):\n    prices = pd.read_csv(\"../input/m5-forecasting-accuracy/sell_prices.csv\", dtype = PRICE_DTYPES)\n    for col, col_dtype in PRICE_DTYPES.items():\n        if col_dtype == \"category\":\n            prices[col] = prices[col].cat.codes.astype(\"int16\")\n            prices[col] -= prices[col].min()\n            \n    cal = pd.read_csv(\"../input/m5-forecasting-accuracy/calendar.csv\", dtype = CAL_DTYPES)\n    cal[\"date\"] = pd.to_datetime(cal[\"date\"])\n    for col, col_dtype in CAL_DTYPES.items():\n        if col_dtype == \"category\":\n            cal[col] = cal[col].cat.codes.astype(\"int16\")\n            cal[col] -= cal[col].min()\n    \n    start_day = max(1 if is_train  else tr_last-max_lags, first_day)\n    numcols = [f\"d_{day}\" for day in range(start_day,tr_last+1)]\n    catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n    dtype = {numcol:\"float32\" for numcol in numcols} \n    dtype.update({col: \"category\" for col in catcols if col != \"id\"})\n    dt = pd.read_csv(\"../input/m5-forecasting-accuracy/sales_train_validation.csv\", \n                     nrows = nrows, usecols = catcols + numcols, dtype = dtype)\n    \n    for col in catcols:\n        if col != \"id\":\n            dt[col] = dt[col].cat.codes.astype(\"int16\")\n            dt[col] -= dt[col].min()\n    \n    if not is_train:\n        for day in range(tr_last+1, tr_last+ 28 +1):\n            dt[f\"d_{day}\"] = np.nan\n    \n    dt = pd.melt(dt,\n                  id_vars = catcols,\n                  value_vars = [col for col in dt.columns if col.startswith(\"d_\")],\n                  var_name = \"d\",\n                  value_name = \"sales\")\n    \n    dt = dt.merge(cal, on= \"d\", copy = False)\n    dt = dt.merge(prices, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], copy = False)\n    \n    return dt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_fea(dt):\n    lags = [7, 28]\n    lag_cols = [f\"lag_{lag}\" for lag in lags ]\n    for lag, lag_col in zip(lags, lag_cols):\n        dt[lag_col] = dt[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(lag)\n\n    wins = [7, 28]\n    for win in wins :\n        for lag,lag_col in zip(lags, lag_cols):\n            dt[f\"rmean_{lag}_{win}\"] = dt[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).mean())\n\n    \n    \n    date_features = {\n        \n        \"wday\": \"weekday\",\n        \"week\": \"weekofyear\",\n        \"month\": \"month\",\n        \"quarter\": \"quarter\",\n        \"year\": \"year\",\n        \"mday\": \"day\",\n#         \"ime\": \"is_month_end\",\n#         \"ims\": \"is_month_start\",\n    }\n    \n#     dt.drop([\"d\", \"wm_yr_wk\", \"weekday\"], axis=1, inplace = True)\n    \n    for date_feat_name, date_feat_func in date_features.items():\n        if date_feat_name in dt.columns:\n            dt[date_feat_name] = dt[date_feat_name].astype(\"int16\")\n        else:\n            dt[date_feat_name] = getattr(dt[\"date\"].dt, date_feat_func).astype(\"int16\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FIRST_DAY = 350 # If you want to load all the data set it to '1' -->  Great  memory overflow  risk !","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndf = create_dt(is_train=True, first_day= FIRST_DAY)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ncreate_fea(df)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(inplace = True)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_feats = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id'] + [\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\"]\nuseless_cols = [\"id\", \"date\", \"sales\",\"d\", \"wm_yr_wk\", \"weekday\"]\ntrain_cols = df.columns[~df.columns.isin(useless_cols)]\nX_train = df[train_cols]\ny_train = df[\"sales\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nnp.random.seed(777)\n\nfake_valid_inds = np.random.choice(X_train.index.values, 2_000_000, replace = False)\ntrain_inds = np.setdiff1d(X_train.index.values, fake_valid_inds)\nX_train=X_train.loc[fake_valid_inds]\ny_train = y_train.loc[fake_valid_inds]\nX_traine=X_train.loc[fake_valid_inds]\ny_traine = y_train.loc[fake_valid_inds]\n# train_data = lgb.Dataset(X_train.loc[train_inds] , label = y_train.loc[train_inds], \n#                          categorical_feature=cat_feats, free_raw_data=False)\n# fake_valid_data = lgb.Dataset(X_train.loc[fake_valid_inds], label = y_train.loc[fake_valid_inds],\n#                               categorical_feature=cat_feats,\n#                  free_raw_data=False)# This is a random sample, we're not gonna apply any time series train-test-split tricks here!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#del df, X_train, y_train, fake_valid_inds,train_inds ; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom mlxtend.regressor import StackingCVRegressor\nlightgbm1 = LGBMRegressor(objective='poisson', \n                       metric ='rmse',\n                       learning_rate = 0.075,\n                       sub_row = 0.75,\n                       bagging_freq = 1,\n                       lambda_l2 = 0.1,\n                       verbosity= 1,\n                       num_iterations = 2000,\n                       num_leaves= 128,\n                       min_data_in_leaf= 100)\nlightgbm2 = LGBMRegressor(objective='tweedie', \n                       metric ='rmse',\n                       learning_rate = 0.075,\n                       sub_row = 0.75,\n                       bagging_freq = 1,\n                       lambda_l2 = 0.1,\n                       verbosity= 1,\n                       num_iterations = 2000,\n                       num_leaves= 128,\n                       min_data_in_leaf= 100)\n\nlightgbm3 = LGBMRegressor(objective='poisson', \n                       metric ='rmse',\n                       learning_rate = 0.075,\n                       sub_row = 0.75,\n                       bagging_freq = 1,\n                       lambda_l2 = 0.1,\n                       verbosity= 1,\n                       num_iterations = 1200,\n                       num_leaves= 128,\n                       min_data_in_leaf= 100)\nlightgbm4 = LGBMRegressor(objective='tweedie', \n                       metric ='rmse',\n                       learning_rate = 0.075,\n                       sub_row = 0.75,\n                       bagging_freq = 1,\n                       lambda_l2 = 0.1,\n                       verbosity= 1,\n                       num_iterations = 1500,\n                       num_leaves= 128,\n                       min_data_in_leaf= 100)\n\nxgboost = XGBRegressor(objective='count:poisson',\n                       learning_rate=0.075,\n                       n_estimators=1000,\n                       min_child_weight=50)\n\nstackReg = StackingCVRegressor(regressors=(lightgbm1,lightgbm2,lightgbm3,lightgbm4),\n                                meta_regressor=(lightgbm1),\n                                use_features_in_secondary=True, \n                                random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def stacked_ensemble(X_train,y_train):\n#     estimators = [\n#     ('rfr', RandomForestRegressor(n_estimators=150)),\n#     ('adb', AdaBoostRegressor(n_estimators=150)),\n#     ('etr',ExtraTreesRegressor(n_estimators=150)),\n#     ('gbr',GradientBoostingRegressor(n_estimators=150)),\n#     ('bar',BaggingRegressor(n_estimators=150))\n#     ]\n#     reg = StackingRegressor(estimators=estimators,\n#                         final_estimator=XGBRegressor(n_estimators=300,learning_rate=0.05))\n#     reg.fit(X_train,y_train)\n#     return(reg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#my_model = stacked_ensemble(X_train,y_train)\nimport warnings\nwarnings.filterwarnings(\"default\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nm_lgb= stackReg.fit(X_train, y_train)\n                   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction stage\n(updated vs original)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_lag_features_for_test(dt, day):\n    # create lag feaures just for single day (faster)\n    lags = [7, 28]\n    lag_cols = [f\"lag_{lag}\" for lag in lags]\n    for lag, lag_col in zip(lags, lag_cols):\n        dt.loc[dt.date == day, lag_col] = \\\n            dt.loc[dt.date ==day-timedelta(days=lag), 'sales'].values  # !!! main\n\n    windows = [7, 28]\n    for window in windows:\n        for lag in lags:\n            df_window = dt[(dt.date <= day-timedelta(days=lag)) & (dt.date > day-timedelta(days=lag+window))]\n            df_window_grouped = df_window.groupby(\"id\").agg({'sales':'mean'}).reindex(dt.loc[dt.date==day,'id'])\n            dt.loc[dt.date == day,f\"rmean_{lag}_{window}\"] = \\\n                df_window_grouped.sales.values     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_date_features_for_test(dt):\n    # copy of the code from `create_dt()` above\n    date_features = {\n        \"wday\": \"weekday\",\n        \"week\": \"weekofyear\",\n        \"month\": \"month\",\n        \"quarter\": \"quarter\",\n        \"year\": \"year\",\n        \"mday\": \"day\",\n    }\n\n    for date_feat_name, date_feat_func in date_features.items():\n        if date_feat_name in dt.columns:\n            dt[date_feat_name] = dt[date_feat_name].astype(\"int16\")\n        else:\n            dt[date_feat_name] = getattr(\n                dt[\"date\"].dt, date_feat_func).astype(\"int16\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nalphas = [1.028, 1.023, 1.018]\nweights = [1/len(alphas)]*len(alphas)  # equal weights\n\nte0 = create_dt(False)  # create master copy of `te`\ncreate_date_features_for_test (te0)\n\nfor icount, (alpha, weight) in enumerate(zip(alphas, weights)):\n    te = te0.copy()  # just copy\n#     te1 = te0.copy()\n    cols = [f\"F{i}\" for i in range(1, 29)]\n\n    for tdelta in range(0, 28):\n        day = fday + timedelta(days=tdelta)\n        print(tdelta, day.date())\n        tst = te[(te.date >= day - timedelta(days=max_lags))\n                 & (te.date <= day)].copy()\n#         tst1 = te1[(te1.date >= day - timedelta(days=max_lags))\n#                  & (te1.date <= day)].copy()\n#         create_fea(tst)  # correct, but takes much time\n        create_lag_features_for_test(tst, day)  # faster  \n        tst = tst.loc[tst.date == day, train_cols]\n        te.loc[te.date == day, \"sales\"] = \\\n            alpha * m_lgb.predict(tst)  # magic multiplier by kyakovlev\n        \n#         create_lag_features_for_test(tst1, day)  # faster  \n#         tst1 = tst1.loc[tst1.date == day, train_cols]\n#         te1.loc[te1.date == day, \"sales\"] = \\\n#             alpha * m_lgb1.predict(tst1)  # magic multiplier by kyakovlev\n\n    te_sub = te.loc[te.date >= fday, [\"id\", \"sales\"]].copy()\n#     te_sub1 = te1.loc[te1.date >= fday, [\"id\", \"sales\"]].copy()\n\n    te_sub[\"F\"] = [f\"F{rank}\" for rank in te_sub.groupby(\"id\")[\n        \"id\"].cumcount()+1]\n#     te_sub1[\"F\"] = [f\"F{rank}\" for rank in te_sub1.groupby(\"id\")[\n#         \"id\"].cumcount()+1]\n    te_sub = te_sub.set_index([\"id\", \"F\"]).unstack()[\n        \"sales\"][cols].reset_index()\n#     te_sub1 = te_sub1.set_index([\"id\", \"F\"]).unstack()[\n#         \"sales\"][cols].reset_index()\n    \n    te_sub.fillna(0., inplace=True)\n#     te_sub1.fillna(0., inplace=True)\n    te_sub.sort_values(\"id\", inplace=True)\n#     te_sub1.sort_values(\"id\", inplace=True)\n    te_sub.reset_index(drop=True, inplace=True)\n#     te_sub1.reset_index(drop=True, inplace=True)\n    te_sub.to_csv(f\"submission_{icount}.csv\", index=False)\n#     te_sub1.to_csv(f\"submission1_{icount}.csv\", index=False)\n    if icount == 0:\n        sub = te_sub\n        sub[cols] *= weight\n#         sub1 = te_sub1\n#         sub1[cols] *= weight\n    else:\n        sub[cols] += te_sub[cols]*weight\n#         sub1[cols] += te_sub1[cols]*weight\n    print(icount, alpha, weight)\n    \n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.id.nunique(), sub[\"id\"].str.contains(\"validation$\").sum()\n# sub1.id.nunique(), sub1[\"id\"].str.contains(\"validation$\").sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.shape\n# sub1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub2 = sub.copy()\nsub2[\"id\"] = sub2[\"id\"].str.replace(\"validation$\", \"evaluation\")\nsub = pd.concat([sub, sub2], axis=0, sort=False)\nsub.to_csv(\"submissionp.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sub3 = sub1.copy()\n# sub3[\"id\"] = sub3[\"id\"].str.replace(\"validation$\", \"evaluation\")\n# sub1 = pd.concat([sub1, sub3], axis=0, sort=False)\n# sub.to_csv(\"submissiont.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# poisson = sub.sort_values(by = 'id').reset_index(drop = True)\n# tweedie = sub1.sort_values(by = 'id').reset_index(drop = True)\n# sub5 = poisson.copy()\n\n# for i in sub5.columns :\n#     if i != 'id' :\n#         sub5[i] = 0.5*poisson[i] + 0.5*tweedie[i]\n        \n# sub5.to_csv('submissionavg.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}